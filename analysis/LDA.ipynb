{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e335b49e",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "This notebook provides all code needed to calculate and visualize the Latent Dirichlet Allocation (LDA) for the FACTROID Data Set\n",
    "\n",
    "The code is based on this article: https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "\n",
    "packages needed:\n",
    "```\n",
    "pip install pandas numpy matplotlib tqdm gensim nltk pyLDAvis\n",
    "```\n",
    "\n",
    "Make sure to also download the spacy modul\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7764f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# needed for calculation of LDA and pre-processing\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# visualization of LDA\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have never downloaded stopwords run this cell to get the nltk stopwords. This should not take long\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c791a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e861507",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/reddit_corpus_balanced_filtered.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42255d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all labeled post with exactly one link\n",
    "data = []\n",
    "for row in df.iterrows():\n",
    "    d = row[1]\n",
    "    for doc in d['documents']:\n",
    "        if len(doc[4]) == 1:\n",
    "            data.append(doc[1])\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be6d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will preprocess the text and \n",
    "\n",
    "data = [re.sub(r'http\\S+', '', sent, flags=re.MULTILINE) for sent in data]\n",
    "data = [re.sub(r'\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"[\\'\\\"\\*\\@()\\[\\]]\", \"\", sent) for sent in data]\n",
    "\n",
    "print('Basic cleaning done')\n",
    "\n",
    "#cleaning the text \n",
    "def tokeniz(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "processed_data = list(tokeniz(data))\n",
    "\n",
    "#Building Bigram & Trigram Models\n",
    "bigram = gensim.models.Phrases(processed_data, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "print('Bi and Trigrams done')\n",
    "\n",
    "#function to filter out stopwords\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "#function to create bigrams\n",
    "def create_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "#function for lemmatization\n",
    "def lemmatize(texts, allowed_postags=['NOUN', 'ADJ', 'VERB']):\n",
    "    texts_op = []\n",
    "    for sent in tqdm(texts, desc='Lemmatize'):\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_op.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_op\n",
    "\n",
    "#removing stopwords, creating bigrams and lemmatizing the text\n",
    "data_wo_stopwords = remove_stopwords(processed_data)\n",
    "print('Stopwords done')\n",
    "data_bigrams = create_bigrams(data_wo_stopwords)\n",
    "print('Bigrams done')\n",
    "data_lemmatized = lemmatize(data_bigrams)\n",
    "\n",
    "\n",
    "#creating a dictionary\n",
    "gensim_dictionary = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "\n",
    "#building a corpus for the topic model\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#printing the corpus we created above.\n",
    "print(gensim_corpus[:3]) \n",
    "\n",
    "#we can print the words with their frequencies.\n",
    "print([[(gensim_dictionary[id], freq) for id, freq in cp] for cp in gensim_corpus[:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f7987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the LDA for different number of topics. Here 2 to 11.\n",
    "# The .txt files are saveed for later use\n",
    "ldas = []\n",
    "for i in tqdm(range(2, 12)):\n",
    "    #creating the LDA model \n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=gensim_corpus, id2word=gensim_dictionary, num_topics=i, random_state=100, \n",
    "        update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True\n",
    "        )\n",
    "    lda_model.save('ldas/lda_' + str(i) + '.model')\n",
    "    ldas.append(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c33df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDA can visualize the gensim model easily.\n",
    "# When using 9 or more topics we see that some topics are (almost) subsets of other topics.\n",
    "# Therefor we pick 8 Topics.\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(ldas[7], gensim_corpus, gensim_dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4149cfad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# saving everything to json files\n",
    "for ind, lda in enumerate(ldas):\n",
    "    vis = gensimvis.prepare(lda, gensim_corpus, gensim_dictionary)\n",
    "    pyLDAvis.save_json(vis, 'ldas/pyLDAvis' + str(ind + 2) + '.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pandas\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce61b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ldas/pyLDAvis8.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "cloud_data = pd.DataFrame(data['tinfo'])\n",
    "circle_data = data['mdsDat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c82fae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = len(circle_data['topics'])\n",
    "# creating grid for subplots\n",
    "\n",
    "fig = plt.figure(figsize=(16,8.5))\n",
    "\n",
    "# All the axis\n",
    "ax1 = plt.subplot2grid(shape=(4, 8), loc=(0, 0), colspan=4, rowspan=4)\n",
    "wordaxes = []\n",
    "for i in range(4):\n",
    "    for j in [4, 6]:\n",
    "        wordaxes.append(plt.subplot2grid(shape=(4, 8), loc=(i, j), colspan=2))\n",
    "\n",
    "x_max = max(circle_data['x'])*1.4\n",
    "y_max = max(circle_data['y'])*1.4\n",
    "m = max(x_max, y_max)\n",
    "ax1.set_xlim(-m-0.09, m-0.09)\n",
    "ax1.set_ylim(-m, m)\n",
    "ax1.plot([-0.35, 0.2], [0, 0], c='gray', alpha=0.7)\n",
    "ax1.plot([0, 0],[-0.3, 0.3], c='gray', alpha=0.7)\n",
    "ax1.text(-0.35, -0.02, 'PCA1', c='gray')\n",
    "ax1.text(-0.02, -0.295, 'PCA2', c='gray', rotation=90)\n",
    "\n",
    "m_freq = max(circle_data['Freq'])\n",
    "\n",
    "# plotting topic circles\n",
    "colors = ['gray', 'purple', 'blue', 'green', 'orange', 'red', 'pink', 'cyan']\n",
    "for i in range(n):\n",
    "    ax1.add_patch(plt.Circle((circle_data['x'][i],circle_data['y'][i]), circle_data['Freq'][i]/(3*m_freq/m), alpha=0.5, color=colors[i]))\n",
    "    ax1.text(circle_data['x'][i],circle_data['y'][i],str(i + 1), ha='center', va='center', size=100*(circle_data['Freq'][i]**0.5)/m_freq)\n",
    "ax1.set_title('Intertopic Distance Map')\n",
    "ax1.axis('off')\n",
    "\n",
    "colormaps = ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds', 'spring', 'cool']\n",
    "for ind, ax in enumerate(wordaxes):\n",
    "    t = cloud_data[cloud_data['Category'] == 'Topic' + str(ind + 1)]\n",
    "    wc = WordCloud(mode = \"RGBA\", background_color=None, colormap=colormaps[ind]).generate_from_frequencies(frequencies=dict(t.iloc[:,0:2].values))\n",
    "    ax.imshow(wc)\n",
    "    ax.set_title('Topic ' + str(ind + 1))\n",
    "    ax.axis('off')\n",
    "\n",
    "fig.suptitle('LDA Visualization for 8 Topics')\n",
    "# automatically adjust padding horizontally\n",
    "# as well as vertically.\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../plots/LDA.pdf')\n",
    "# display plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
