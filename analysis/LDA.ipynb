{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7764f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# needed for calculation of LDA and pre-processing\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# visualization of LDA\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c791a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e861507",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/reddit_corpus_balanced_filtered.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42255d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for row in df.iterrows():\n",
    "    d = row[1]\n",
    "    for doc in d['documents']:\n",
    "        if len(doc[4]) == 1:\n",
    "            data.append(doc[1])\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be6d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "print('Basic cleaning done')\n",
    "\n",
    "#cleaning the text \n",
    "def tokeniz(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "processed_data = list(tokeniz(data))\n",
    "\n",
    "#Building Bigram & Trigram Models\n",
    "bigram = gensim.models.Phrases(processed_data, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "print('Bi and Trigrams done')\n",
    "\n",
    "#function to filter out stopwords\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "#function to create bigrams\n",
    "def create_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "#function for lemmatization\n",
    "def lemmatize(texts, allowed_postags=['NOUN', 'ADJ', 'VERB']):\n",
    "    texts_op = []\n",
    "    for sent in tqdm(texts, desc='Lemmatize'):\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_op.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_op\n",
    "\n",
    "#removing stopwords, creating bigrams and lemmatizing the text\n",
    "data_wo_stopwords = remove_stopwords(processed_data)\n",
    "print('Stopwords done')\n",
    "data_bigrams = create_bigrams(data_wo_stopwords)\n",
    "print('Bigrams done')\n",
    "data_lemmatized = lemmatize(data_bigrams, allowed_postags=[ 'NOUN', 'ADJ', 'VERB'])\n",
    "\n",
    "#printing the lemmatized data\n",
    "print(data_lemmatized[:3])\n",
    "\n",
    "#creating a dictionary\n",
    "gensim_dictionary = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "\n",
    "#building a corpus for the topic model\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#printing the corpus we created above.\n",
    "print(gensim_corpus[:3]) \n",
    "\n",
    "#we can print the words with their frequencies.\n",
    "print([[(gensim_dictionary[id], freq) for id, freq in cp] for cp in gensim_corpus[:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f7987",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldas = []\n",
    "for i in tqdm(range(2, 16)):\n",
    "    #creating the LDA model \n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=gensim_corpus, id2word=gensim_dictionary, num_topics=i, random_state=100, \n",
    "        update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True\n",
    "        )\n",
    "    lda_model.save('ldas/lda_' + str(i) + '.model')\n",
    "    ldas.append(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c33df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "gensimvis.prepare(ldas[7], gensim_corpus, gensim_dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
