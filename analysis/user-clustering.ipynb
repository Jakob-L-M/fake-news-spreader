{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Clustering\n",
    "\n",
    "This notebook is used to cluster users based on some vector representation. Additionaly can can calculate clusters based on temporal splits.\n",
    "\n",
    "To run the calcutation cell the notebook requires a pandas dataframe of the following structure:\n",
    "\n",
    "| user_id | fn_news_spreader | pb_factor | factual_factor | vector1 | vector2 | cluster\n",
    "|-|-|-|-|-|-|-|\n",
    "USER_ID1| 0 | 1.21 | 2.34 | [0.2,...,-0.1] | [0.6,...,0.7] | 1\n",
    "USER_ID2| 1 | -0.34 | 1.79 | [0.7,...,0.5] | [-0.1,...,0.0] | 0\n",
    "... | ... | ... | ... | ... | ... | ...\n",
    "\n",
    "Where fn_spreader is a binary variable, pol_bias is some float between [-3, 3] and the vector is a normalized vector of some dimension. The datafram without vectors and no clustering can be found [in this google drive](https://drive.google.com/file/d/1FbkQn2d9LiJ54ZrxiLBAYJd_rCAtn0QM/view?usp=sharing). There are some cells below showing how you can add your own vectors.\n",
    "\n",
    "The clustering will append an additional column to the dataframe containing the cluster label. The function _score_clustering(dataframe)_ will we return a score based on inter cluster similarity. The whole clustering gets a score based on the weighted mean of its clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/blank_user_frame.csv')\n",
    "df = df.fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding named entity vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/named_entities.pickel', 'rb') as f:\n",
    "    user_entities = pickle.load(f)\n",
    "\n",
    "named_entities = ['ORG', 'PERSON', 'DATE', 'GPE','CARDINAL', 'NORP',\n",
    "                  'PERCENT', 'MONEY', 'ORDINAL', 'WORK_OF_ART', 'LOC',\n",
    "                 'TIME', 'LAW', 'PRODUCT', 'FAC', 'EVENT', 'QUANTITY',\n",
    "                 'LANGUAGE']\n",
    "\n",
    "vectors = []\n",
    "for user in tqdm(df['user_id'], total=len(df)):\n",
    "    \n",
    "    ents = user_entities[user]['all']\n",
    "    \n",
    "    # normalization\n",
    "    N = sum(ents.values())\n",
    "    \n",
    "    # preparing vector; 0 as default value\n",
    "    vector = [0]*len(named_entities)\n",
    "    \n",
    "    for ind, entity in enumerate(named_entities):\n",
    "        if entity in ents:\n",
    "            vector[ind] = ents[entity]/N\n",
    "    vectors.append(np.array(vector))\n",
    "\n",
    "df['named_entities'] = vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Linguistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/linguistic_features.pickel', 'rb') as f:\n",
    "    ling_features = pickle.load(f)\n",
    "    \n",
    "features = ['DET', 'NOUN', 'SCONJ', 'AUX', 'PART', 'VERB', 'PRON', 'ADJ', 'PUNCT',\n",
    "            'ADP', 'PROPN', 'NUM', 'CCONJ', 'ADV', 'SPACE', 'SYM', 'INTJ', 'X']\n",
    "\n",
    "vectors = []\n",
    "for user in tqdm(df['user_id'], total=len(df)):\n",
    "    \n",
    "    feat = ling_features[user]['all']\n",
    "    \n",
    "    # normalization\n",
    "    N = sum(feat.values())\n",
    "    \n",
    "    # preparing vector; 0 as default value\n",
    "    vector = [0]*len(features)\n",
    "    \n",
    "    for ind, f in enumerate(features):\n",
    "        if f in feat:\n",
    "            vector[ind] = feat[f]/N\n",
    "    vectors.append(np.array(vector))\n",
    "\n",
    "df['linguistic_features'] = vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding sBert embeddings - by Ezzeddine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/user_embedding_basic_sbert.p', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "vectors = []\n",
    "for user in tqdm(df['user_id'], total=len(df)):\n",
    "    \n",
    "    if user not in embeddings:\n",
    "        vectors.append([0]*768)\n",
    "    else:\n",
    "        vectors.append(np.array(embeddings[user]))\n",
    "\n",
    "df['sBert'] = vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_stats(stats):\n",
    "    return \"\\033[38;2;{};{};{};{}m{}: {}\\033[38;2;0;0;0;0m\".format(round(255-(stats[1])*2.55), round(stats[1]*2.55), 0, 4 if stats[2] == 1 else 2, str(stats[3]), str(stats[0]))\n",
    "\n",
    "def inter_similarity(frame, col):\n",
    "    \n",
    "    if 'mask' in frame.columns:\n",
    "        df = frame[frame['mask']]\n",
    "    else:\n",
    "        df = frame\n",
    "    N = len(df)\n",
    "    \n",
    "    fn_similatity = 0\n",
    "    pb_var = 0\n",
    "    fact_var = 0\n",
    "    \n",
    "    stats = []\n",
    "    \n",
    "    for c in df[col].unique():\n",
    "        \n",
    "        temp = df[df[col] == c]\n",
    "        cluster_size = len(temp)\n",
    "        \n",
    "        pb_var += (cluster_size/N) * np.var(temp['pb_factor'])\n",
    "        fact_var += (cluster_size/N) * np.var(temp['factual_factor'])\n",
    "        \n",
    "        # fn_amount = sum(temp['fake_news_spreader'])\n",
    "        fn_amount = sum(temp['rebalanced'])\n",
    "        \n",
    "        # more fn spreaders than rn spreaders\n",
    "        if fn_amount > 0.5*cluster_size:\n",
    "            score = (cluster_size/N) * (fn_amount/cluster_size)\n",
    "            fn = 1\n",
    "        \n",
    "        # more rn than fn\n",
    "        else:\n",
    "            score = (cluster_size/N) * ((cluster_size - fn_amount)/cluster_size)\n",
    "            fn = 0\n",
    "            \n",
    "        fn_similatity += score\n",
    "        \n",
    "        stats.append((cluster_size, score*(N/cluster_size)*100, fn, c))\n",
    "                    \n",
    "    \n",
    "    print('fn_news_spreader: {0:.2f}%'.format(fn_similatity*100),\n",
    "          'with {} clusters'.format(len(df[col].unique())))\n",
    "    \n",
    "    print('political bias variance: {:.3f}'.format(pb_var))\n",
    "    print('factual factor variance: {:.3f}'.format(fact_var))\n",
    "    \n",
    "    stats = sorted(stats, key=lambda x: x[1])\n",
    "    \n",
    "    s = ''\n",
    "    for stat in stats:\n",
    "        s += format_stats(stat) + ' '\n",
    "    print('Size distribution:', s)\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    return fn_similatity, pb_var, fact_var, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random, a baseline\n",
    "from random import randint\n",
    "\n",
    "for i in range(2, 9):\n",
    "    df['RDcluster' + str(i)] = [randint(0,i) for _ in range(len(df))]\n",
    "    inter_similarity(df, 'RDcluster' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=3).fit(list(df['named_entities']))\n",
    "distances, indices = nbrs.kneighbors(list(df['named_entities']))\n",
    "y = sorted(np.mean(distances, axis=1))\n",
    "plt.plot(range(len(y)), y)\n",
    "plt.ylim([0, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on named_entities\n",
    "\n",
    "for i in range(2, 9):\n",
    "    clustering = KMeans(n_clusters=i, random_state=0).fit(list(df['named_entities']))\n",
    "    df['NEcluster' + str(i)] = clustering.labels_\n",
    "    inter_similarity(df, 'NEcluster' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on linguistic features\n",
    "\n",
    "for i in range(2, 9):\n",
    "    clustering = KMeans(n_clusters=i, random_state=0).fit(list(df['linguistic_features']))\n",
    "    df['lingcluster' + str(i)] = clustering.labels_\n",
    "    inter_similarity(df, 'lingcluster' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on linguistic features and named_entities\n",
    "\n",
    "for i in range(2, 9):\n",
    "    clustering = KMeans(n_clusters=i, random_state=0).fit([list(i[1]['named_entities'])\n",
    "                                                           + list(i[1]['linguistic_features'])\n",
    "                                                           for i in df.iterrows()])\n",
    "    \n",
    "    df['cluster'] = clustering.labels_\n",
    "    inter_similarity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run everything\n",
    "\n",
    "logs = {}\n",
    "\n",
    "# Based on sBert\n",
    "for i in glob('../data/*.p'):\n",
    "    \n",
    "    name = 'sBert_' + i[i.index('user_embedding_')+15:-2]\n",
    "    \n",
    "    mask = []\n",
    "\n",
    "    with open(i, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    vectors = []\n",
    "    for user in tqdm(df['user_id'], total=len(df)):\n",
    "\n",
    "        if user not in embeddings:\n",
    "            vectors.append([0]*768)\n",
    "            mask.append(False)\n",
    "        else:\n",
    "            vectors.append(np.array(embeddings[user]))\n",
    "            mask.append(True)\n",
    "\n",
    "    df['sBert'] = vectors\n",
    "    df['mask'] = mask\n",
    "    \n",
    "    name += '[' + str(sum(mask)) + ']'\n",
    "    \n",
    "    logs[name] = {}\n",
    "    \n",
    "    print('File:', i)\n",
    "\n",
    "    for i in range(2, 12):\n",
    "        \n",
    "        logs[name][i] = {}\n",
    "        \n",
    "        clustering = KMeans(n_clusters=i, random_state=0).fit(list(df['sBert']))\n",
    "        df[name + str(i)] = clustering.labels_\n",
    "        fn_sim, pb_var, fact_var, stats = inter_similarity(df, name + str(i))\n",
    "        \n",
    "        logs[name][i]['fn'] = fn_sim\n",
    "        logs[name][i]['pb'] = pb_var\n",
    "        logs[name][i]['ff'] = fact_var\n",
    "        logs[name][i]['stats'] = stats\n",
    "        \n",
    "# User2Vec\n",
    "with open('../data/user2vec.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "vectors = []\n",
    "mask = []\n",
    "for user in tqdm(df['user_id'], total=len(df)):\n",
    "\n",
    "    if user not in embeddings:\n",
    "        vectors.append([0]*200)\n",
    "        mask.append(False)\n",
    "    else:\n",
    "        vectors.append(np.array(embeddings[user]))\n",
    "        mask.append(True)\n",
    "\n",
    "df['mask'] = mask\n",
    "df['user2vec'] = vectors\n",
    "\n",
    "name = 'user2vec[' + str(sum(mask)) + ']'\n",
    "logs[name] = {}\n",
    "\n",
    "print('File:', i)\n",
    "\n",
    "for i in range(2, 12):\n",
    "\n",
    "    logs[name][i] = {}\n",
    "\n",
    "    clustering = KMeans(n_clusters=i, random_state=0).fit(list(df['user2vec']))\n",
    "    df[name + str(i)] = clustering.labels_\n",
    "    fn_sim, pb_var, fact_var, stats = inter_similarity(df, name + str(i))\n",
    "\n",
    "    logs[name][i]['fn'] = fn_sim\n",
    "    logs[name][i]['pb'] = pb_var\n",
    "    logs[name][i]['ff'] = fact_var\n",
    "    logs[name][i]['stats'] = stats\n",
    "    \n",
    "\n",
    "# Named Entities and ling features\n",
    "name = 'named_entities[' + str(len(df)) + ']'\n",
    "logs[name] = {}\n",
    "for i in range(2, 12):\n",
    "\n",
    "    logs[name][i] = {}\n",
    "\n",
    "    clustering = KMeans(n_clusters=i, random_state=0).fit(list(df['named_entities']))\n",
    "    df[name + str(i)] = clustering.labels_\n",
    "    fn_sim, pb_var, fact_var, stats = inter_similarity(df, name + str(i))\n",
    "\n",
    "    logs[name][i]['fn'] = fn_sim\n",
    "    logs[name][i]['pb'] = pb_var\n",
    "    logs[name][i]['ff'] = fact_var\n",
    "    logs[name][i]['stats'] = stats\n",
    "\n",
    "name = 'linguistic_features[' + str(len(df)) + ']'\n",
    "logs[name] = {}\n",
    "for i in range(2, 12):\n",
    "\n",
    "    logs[name][i] = {}\n",
    "\n",
    "    clustering = KMeans(n_clusters=i, random_state=0).fit(list(df['linguistic_features']))\n",
    "    df[name + str(i)] = clustering.labels_\n",
    "    fn_sim, pb_var, fact_var, stats = inter_similarity(df, name + str(i))\n",
    "\n",
    "    logs[name][i]['fn'] = fn_sim\n",
    "    logs[name][i]['pb'] = pb_var\n",
    "    logs[name][i]['ff'] = fact_var\n",
    "    logs[name][i]['stats'] = stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing differneces of sBert fine tunings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [i for i in logs.keys() if 'dicard' not in i]\n",
    "sizes = list(logs[rows[0]].keys())\n",
    "\n",
    "fn_df = pd.DataFrame(columns=sizes)\n",
    "pb_df = pd.DataFrame(columns=sizes)\n",
    "ff_df = pd.DataFrame(columns=sizes)\n",
    "\n",
    "for r in rows:\n",
    "    fn_df = fn_df.append({s:logs[r][s]['fn'] for s in sizes}, ignore_index=True)\n",
    "    pb_df = pb_df.append({s:logs[r][s]['pb'] for s in sizes}, ignore_index=True)\n",
    "    ff_df = ff_df.append({s:logs[r][s]['ff'] for s in sizes}, ignore_index=True)\n",
    "    \n",
    "fn_df.index = rows\n",
    "pb_df.index = rows\n",
    "ff_df.index = rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(fn_df, cmap='turbo',linewidths=.5)\n",
    "ax.set(xlabel='Number of clusters', ylabel='Embedding', title='Inter-Cluster similarity of fn and rn spreaders');\n",
    "\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('../analysis/cluster_fn_sim_rebalanced_wo_disc.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(pb_df, cmap='turbo',linewidths=.5)\n",
    "ax.set(xlabel='Number of clusters', ylabel='Embedding', title='Inter-Cluster political bias variance');\n",
    "\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('../analysis/cluster_pol_bias_rebalanced_wo_disc.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(ff_df, cmap='turbo',linewidths=.5)\n",
    "ax.set(xlabel='Number of clusters', ylabel='Embedding', title='Inter-Cluster factual factor variance');\n",
    "\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('../analysis/cluster_fact_fac_rebalanced_wo_disc.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to topics influence the different clusterings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit: https://stackoverflow.com/questions/46131572/making-a-non-overlapping-bubble-chart-in-matplotlib-circle-packing\n",
    "class C():\n",
    "    def __init__(self,r):\n",
    "        self.N = len(r)\n",
    "        self.x = np.ones((self.N,3))\n",
    "        self.x[:,2] = r\n",
    "        maxstep = 2*self.x[:,2].max()\n",
    "        length = np.ceil(np.sqrt(self.N))\n",
    "        grid = np.arange(0,length*maxstep,maxstep)\n",
    "        gx,gy = np.meshgrid(grid,grid)\n",
    "        self.x[:,0] = gx.flatten()[:self.N]\n",
    "        self.x[:,1] = gy.flatten()[:self.N]\n",
    "        self.x[:,:2] = self.x[:,:2] - np.mean(self.x[:,:2], axis=0)\n",
    "\n",
    "        self.step = self.x[:,2].min()\n",
    "        self.p = lambda x,y: np.sum((x**2+y**2)**2)\n",
    "        self.E = self.energy()\n",
    "        self.iter = 1.\n",
    "\n",
    "    def minimize(self):\n",
    "        while self.iter < 500*self.N:\n",
    "            for i in range(self.N):\n",
    "                rand = np.random.randn(2)*self.step/self.iter\n",
    "                self.x[i,:2] += rand\n",
    "                e = self.energy()\n",
    "                if (e < self.E and self.isvalid(i)):\n",
    "                    self.E = e\n",
    "                    self.iter = 1.\n",
    "                else:\n",
    "                    self.x[i,:2] -= rand\n",
    "                    self.iter += 1.\n",
    "\n",
    "    def energy(self):\n",
    "        return self.p(self.x[:,0], self.x[:,1])\n",
    "\n",
    "    def distance(self,x1,x2):\n",
    "        return np.sqrt((x1[0]-x2[0])**2+(x1[1]-x2[1])**2)-x1[2]-x2[2]\n",
    "\n",
    "    def isvalid(self, i):\n",
    "        for j in range(self.N):\n",
    "            if i!=j: \n",
    "                if self.distance(self.x[i,:], self.x[j,:]) < 0:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def plot(self, ax):\n",
    "        for i in range(self.N):\n",
    "            circ = plt.Circle(self.x[i,:2],self.x[i,2] )\n",
    "            ax.add_patch(circ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_dist(embedding):\n",
    "    topics = ['general-political-debate', 'Vaccines', 'SARS-CoV-2', \n",
    "                  'womens-and-mens-rights', 'Gun-control', 'Climate-change',\n",
    "                  '5G', 'Abortion']\n",
    "    topic_dists = [[0]*len(topics) for _ in range(len(df[embedding].unique()))]\n",
    "    for i in df[embedding].unique():\n",
    "        cluster = df[df[embedding] == i]\n",
    "        dic = dict(cluster['topic'].value_counts())\n",
    "        for ind, t in enumerate(topics):\n",
    "            if t in dic:\n",
    "                topic_dists[i][ind] = dic[t]\n",
    "    return topic_dists\n",
    "\n",
    "def make_pie_plot(name, rating, fn, positions, topics):\n",
    "    _topics = ['general-political-debate', 'Vaccines', 'SARS-CoV-2', \n",
    "                  'womens-and-mens-rights', 'Gun-control', 'Climate-change',\n",
    "                  '5G', 'Abortion']\n",
    "    _colors = ['orangered', 'lime', 'aqua', 'violet', 'gold', 'grey', 'blue', 'darkmagenta']\n",
    "    \n",
    "    n_clusters = len(positions)\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        plt.pie(topics[i], radius=positions[i][2]*0.95,\n",
    "                center=positions[i][:2], colors=_colors, startangle=45,\n",
    "               wedgeprops={'alpha': rating[i]})\n",
    "        plt.gca().add_patch(plt.Circle(radius=positions[i][2]*0.35,\n",
    "                xy=positions[i][:2], color='w'))\n",
    "        plt.gca().add_patch(plt.Circle(radius=positions[i][2]*0.15,\n",
    "                xy=positions[i][:2], facecolor='black' if fn[i] else 'white', edgecolor='white' if fn[i] else 'black'))\n",
    "    plt.title('Topic distributions in ' + name + '\\nwith ' + str(n_clusters) + 'clusters')\n",
    "    \n",
    "    l = plt.legend(_topics + ['real-news', 'fake-news'], bbox_to_anchor=(1, 1))\n",
    "    news_legend = [['white', 'black'], ['black', 'white']]\n",
    "    for ind, text in enumerate(l.legendHandles):\n",
    "        if ind < len(_colors):\n",
    "            text.set_facecolor(_colors[ind])\n",
    "            text.set_alpha(1)\n",
    "        else:\n",
    "            text.set_facecolor(news_legend[ind - len(_colors)][0])\n",
    "            text.set_edgecolor(news_legend[ind - len(_colors)][1])\n",
    "            text.set_alpha(1)\n",
    "    \n",
    "    plt.autoscale()\n",
    "    plt.savefig('cluster-topic-dist/' + name + str(len(sizes)) + '.pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for emb in logs:\n",
    "    for size in logs[emb]:\n",
    "        n = emb[:emb.index('[')]\n",
    "        sizes = [i[0] for i in logs[emb][size]['stats']]\n",
    "        rating = [i[1]/100 for i in logs[emb][size]['stats']]\n",
    "        fn = [bool(i[2]) for i in logs[emb][size]['stats']]\n",
    "        _s = np.log2(sizes)+1\n",
    "        c = C(2*_s/np.sum(_s))\n",
    "        c.minimize()\n",
    "        topics = get_topic_dist(emb + str(size))\n",
    "        make_pie_plot(n, rating, fn, c.x, topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
