{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e90b46",
   "metadata": {},
   "source": [
    "# User Clustering\n",
    "\n",
    "This notebook is used to cluster users based on some vector representation. Additionaly can can calculate clusters based on temporal splits.\n",
    "\n",
    "To run the calcutation cell the notebook requires a pandas dataframe of the following structure:\n",
    "\n",
    "| user_id | fn_news_spreader | pb_factor | factual_factor | vector1 | vector2 | cluster\n",
    "|-|-|-|-|-|-|-|\n",
    "USER_ID1| 0 | 1.21 | 2.34 | [0.2,...,-0.1] | [0.6,...,0.7] | 1\n",
    "USER_ID2| 1 | -0.34 | 1.79 | [0.7,...,0.5] | [-0.1,...,0.0] | 0\n",
    "... | ... | ... | ... | ... | ... | ...\n",
    "\n",
    "Where fn_spreader is a binary variable, pol_bias is some float between [-3, 3] and the vector is a normalized vector of some dimension. The datafram without vectors and no clustering can be found [in this google drive](https://drive.google.com/file/d/1FbkQn2d9LiJ54ZrxiLBAYJd_rCAtn0QM/view?usp=sharing). There are some cells below showing how you can add your own vectors.\n",
    "\n",
    "The clustering will append an additional column to the dataframe containing the cluster label. The function _score_clustering(dataframe)_ will we return a score based on inter cluster similarity. The whole clustering gets a score based on the weighted mean of its clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362d95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1561d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/blank_user_frame.csv', index_col=0)\n",
    "df = df.fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a3c20",
   "metadata": {},
   "source": [
    "## Adding named entity vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e04eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/named_entities.pickel', 'rb') as f:\n",
    "    user_entities = pickle.load(f)\n",
    "\n",
    "named_entities = ['ORG', 'PERSON', 'DATE', 'GPE','CARDINAL', 'NORP',\n",
    "                  'PERCENT', 'MONEY', 'ORDINAL', 'WORK_OF_ART', 'LOC',\n",
    "                 'TIME', 'LAW', 'PRODUCT', 'FAC', 'EVENT', 'QUANTITY',\n",
    "                 'LANGUAGE']\n",
    "\n",
    "vectors = []\n",
    "for user in tqdm(df['user_id'], total=len(df)):\n",
    "    \n",
    "    ents = user_entities[user]['all']\n",
    "    \n",
    "    # normalization\n",
    "    N = sum(ents.values())\n",
    "    \n",
    "    # preparing vector; 0 as default value\n",
    "    vector = [0]*len(named_entities)\n",
    "    \n",
    "    for ind, entity in enumerate(named_entities):\n",
    "        if entity in ents:\n",
    "            vector[ind] = ents[entity]/N\n",
    "    vectors.append(np.array(vector))\n",
    "\n",
    "df['named_entities'] = vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6c182",
   "metadata": {},
   "source": [
    "## Adding Linguistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/linguistic_features.pickel', 'rb') as f:\n",
    "    ling_features = pickle.load(f)\n",
    "    \n",
    "features = ['DET', 'NOUN', 'SCONJ', 'AUX', 'PART', 'VERB', 'PRON', 'ADJ', 'PUNCT',\n",
    "            'ADP', 'PROPN', 'NUM', 'CCONJ', 'ADV', 'SPACE', 'SYM', 'INTJ', 'X']\n",
    "\n",
    "vectors = []\n",
    "for user in tqdm(df['user_id'], total=len(df)):\n",
    "    \n",
    "    feat = ling_features[user]['all']\n",
    "    \n",
    "    # normalization\n",
    "    N = sum(feat.values())\n",
    "    \n",
    "    # preparing vector; 0 as default value\n",
    "    vector = [0]*len(features)\n",
    "    \n",
    "    for ind, f in enumerate(features):\n",
    "        if f in feat:\n",
    "            vector[ind] = feat[f]/N\n",
    "    vectors.append(np.array(vector))\n",
    "\n",
    "df['linguistic_features'] = vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd83cb",
   "metadata": {},
   "source": [
    "## Adding sBert embeddings - by Ezzeddine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c3ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/user_embedding_basic_sbert.p', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "vectors = []\n",
    "for user in tqdm(df['user_id'], total=len(df)):\n",
    "    \n",
    "    if user not in embeddings:\n",
    "        vectors.append([0]*768)\n",
    "    else:\n",
    "        vectors.append(np.array(embeddings[user]))\n",
    "\n",
    "df['sBert'] = vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c38204",
   "metadata": {},
   "source": [
    "## Clustering evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82476327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_similarity(df):\n",
    "    N = len(df)\n",
    "    \n",
    "    fn_similatity = 0\n",
    "    \n",
    "    stats = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for c in df['cluster'].unique():\n",
    "        \n",
    "        temp = df[df['cluster'] == c]\n",
    "        \n",
    "        fn_amount = sum(temp['fake_news_spreader'])\n",
    "        cluster_size = len(temp)\n",
    "        \n",
    "        # more fn spreaders than rn spreaders\n",
    "        if fn_amount > 0.5*cluster_size:\n",
    "            score = 2 * (cluster_size/N) * (fn_amount/cluster_size) - (cluster_size/N)\n",
    "            fn = 1\n",
    "        \n",
    "        # more rn than fn\n",
    "        else:\n",
    "            score = 2 * (cluster_size/N) * ((cluster_size - fn_amount)/cluster_size) - (cluster_size/N)\n",
    "            fn = 0\n",
    "            \n",
    "        fn_similatity += score\n",
    "        \n",
    "        stats.append((cluster_size, score*(N/cluster_size)*100, fn))\n",
    "                    \n",
    "    \n",
    "    print('Inter-Cluster similarity in fn_news_spreader is {0:.2f}%'.format(fn_similatity*100),\n",
    "          'with {} clusters'.format(len(df['cluster'].unique())))\n",
    "    \n",
    "    s = ''\n",
    "    for stat in sorted(stats):\n",
    "        s += colored(round(255-(stat[1])*2.55), round(stat[1]*2.55), 0, str(stat[0])) + ' '\n",
    "    print('Size distribution:', s)\n",
    "    \n",
    "    best = sorted(stats)[0]\n",
    "    print('Best | Size:', best[0], 'Score:', '{0:.2f}%'.format(best[1]), 'Fn?', 'Yes' if best[2] == 1 else 'No')\n",
    "    \n",
    "    worst = sorted(stats)[-1]\n",
    "    print('Worst | Size:', worst[0], 'Score:', '{0:.2f}%'.format(worst[1]), 'Fn?', 'Yes' if worst[2] == 1 else 'No')\n",
    "    \n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31930670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randome, a baseline\n",
    "from random import randint\n",
    "\n",
    "for i in range(2, 9):\n",
    "    df['cluster'] = [randint(0,i) for _ in range(len(df))]\n",
    "    inter_similarity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ec536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on named_entities\n",
    "\n",
    "for i in range(2, 9):\n",
    "    clustering = KMeans(n_clusters=i, random_state=0).fit(list(df['named_entities']))\n",
    "    df['cluster'] = clustering.labels_\n",
    "    inter_similarity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff0bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on linguistic features\n",
    "\n",
    "for i in range(2, 9):\n",
    "    clustering = KMeans(n_clusters=i, random_state=0).fit(list(df['linguistic_features']))\n",
    "    df['cluster'] = clustering.labels_\n",
    "    inter_similarity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e6e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on linguistic features and named_entities\n",
    "\n",
    "for i in range(2, 9):\n",
    "    clustering = KMeans(n_clusters=i, random_state=0).fit([list(i[1]['named_entities'])\n",
    "                                                           + list(i[1]['linguistic_features'])\n",
    "                                                           for i in df.iterrows()])\n",
    "    \n",
    "    df['cluster'] = clustering.labels_\n",
    "    inter_similarity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9a4a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on sBert\n",
    "\n",
    "for i in range(2, 9):\n",
    "    clustering = KMeans(n_clusters=i, random_state=0).fit(list(df['sBert']))\n",
    "    df['cluster'] = clustering.labels_\n",
    "    inter_similarity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06367b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on pol_bias and fact_factor\n",
    "\n",
    "for i in range(2, 20):\n",
    "    clustering = KMeans(n_clusters=i, random_state=0).fit(list(df.iloc[:,[3,4]].to_numpy()))\n",
    "    df['cluster'] = clustering.labels_\n",
    "    inter_similarity(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
