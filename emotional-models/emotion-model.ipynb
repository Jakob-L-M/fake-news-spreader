{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8310eb",
   "metadata": {},
   "source": [
    "### Notebook to train dual emotion models and some plots\n",
    "\n",
    "Make sure to run the emotion-extraction notebook first!\n",
    "Your dataset needs to have the columns 'comment_emotion', 'main_emotion' and 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e491bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from random import random\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dropout, Input, Dense, Softmax, LeakyReLU\n",
    "\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f943a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('factroid_with_emotions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4520c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dual emotion features:\n",
    "comment_emotions = df['comment_emotion'].values\n",
    "main_emotions = df['main_emotion'].values\n",
    "\n",
    "mean_pol_comments = np.array([np.mean(i, axis=0) for i in comment_emotions])\n",
    "max_pol_comments = np.array([np.max(i, axis=0) for i in comment_emotions])\n",
    "\n",
    "mean_gap = np.array([v - mean_pol_comments[ind] for ind, v in enumerate(main_emotions)])\n",
    "max_gap = np.array([v - max_pol_comments[ind] for ind, v in enumerate(main_emotions)])\n",
    "\n",
    "dual_emotion_features = np.array([np.append(main_emotions[i],\n",
    "                                   np.append(mean_pol_comments[i],\n",
    "                                             np.append(max_pol_comments[i],\n",
    "                                                       np.append(mean_gap[i], max_gap[i]))))\n",
    "                                   for i in range(len(main_emotions))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract meaning embeddings\n",
    "# Only run this cell if you never did it before; uncommend the code for that\n",
    "\n",
    "\n",
    "# sentences = df['text']\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "# X = model.encode(sentences, show_progress_bar=True)\n",
    "\n",
    "# np.save('text_embeddings.npy', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd25180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in embeddings\n",
    "X = np.load('./text_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b52d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emo = np.append(X, dual_emotion_features, axis=1)\n",
    "y = pd.get_dummies(df['fn']).to_numpy(dtype=np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9c7a5",
   "metadata": {},
   "source": [
    "### Simple MLP with dual-emotion vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b767cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_emo, y, test_size=0.2)\n",
    "\n",
    "states, counts = np.unique(y_train, axis=0, return_counts=True)\n",
    "\n",
    "print('Train size:', len(y_train),'dist:', counts, '\\nTest size:', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05720d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train balanced:\n",
    "# gives better results\n",
    "states, counts = np.unique(y_train, axis=0, return_counts=True)\n",
    "\n",
    "smallest_state = np.argmin(states)\n",
    "\n",
    "for i in range(len(states)):\n",
    "    if i == smallest_state:\n",
    "        continue\n",
    "    \n",
    "    new_y_train = y_train.copy()\n",
    "    new_X_train = X_train.copy()\n",
    "    \n",
    "    mask = np.all(y_train != states[i], axis=1)\n",
    "    new_y_train = new_y_train[mask]\n",
    "    new_X_train = new_X_train[mask]\n",
    "    \n",
    "    mask = np.all(y_train == states[i], axis=1)\n",
    "    y_examples = y_train[mask]\n",
    "    X_examples = X_train[mask]\n",
    "    \n",
    "    permutation = np.random.permutation(len(y_examples))\n",
    "    \n",
    "    # readd the trimmed down examples to y_train and X_train\n",
    "    new_y_train = np.concatenate((new_y_train, (y_examples[permutation])[:counts[smallest_state]]), axis=0)\n",
    "    y_train = new_y_train\n",
    "    \n",
    "    new_X_train = np.concatenate((new_X_train, (X_examples[permutation])[:counts[smallest_state]]), axis=0)\n",
    "    X_train = new_X_train\n",
    "    \n",
    "    # add the remaining examples to the testing arrays\n",
    "#     y_test = np.concatenate((y_test, (y_examples[permutation])[counts[smallest_state]:]), axis=0)\n",
    "#     X_test = np.concatenate((X_test, (X_examples[permutation])[counts[smallest_state]:]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f348f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sizes\n",
    "states, tr_counts = np.unique(y_train, axis=0, return_counts=True)\n",
    "states, te_counts = np.unique(y_test, axis=0, return_counts=True)\n",
    "\n",
    "print('Train size:', len(y_train),'dist:', tr_counts,\n",
    "      '\\nTest size:', len(y_test), 'dist:', te_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2297b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Code from Dual-Emotion Mining Paper:\n",
    "# https://github.com/RMSnow/WWW2021/blob/master/code/model/MLP.py\n",
    "\n",
    "class MLP5Layers:\n",
    "    def __init__(self, input_dim, category_num=2, l2_param=0.01, lr_param=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.category_num = category_num\n",
    "        self.l2_param = l2_param\n",
    "\n",
    "        self.model = self.build()\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer='adam',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def build(self):\n",
    "#         se_input = Input(shape=(self.input_dim,))\n",
    "\n",
    "#         dense1 = Dense(self.input_dim, activation='relu',\n",
    "#                        kernel_regularizer=l2(self.l2_param))(se_input)\n",
    "#         dense2 = Dense(256, activation='relu',\n",
    "#                        kernel_regularizer=l2(self.l2_param))(dense1)\n",
    "#         dense3 = Dense(128, activation='relu',\n",
    "#                        kernel_regularizer=l2(self.l2_param))(dense2)\n",
    "#         dense4 = Dense(64, activation='relu',\n",
    "#                        kernel_regularizer=l2(self.l2_param))(dense3)\n",
    "#         dense5 = Dense(32, activation='relu',\n",
    "#                        kernel_regularizer=l2(self.l2_param))(dense4)\n",
    "#         output = Dense(1, activation='softmax',\n",
    "#                        kernel_regularizer=l2(self.l2_param))(dense5)\n",
    "\n",
    "#         model = Model(inputs=[se_input], outputs=output)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(64, activation=LeakyReLU(alpha=0.1)))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(32, activation=LeakyReLU(alpha=0.1)))\n",
    "\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1a15f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MLP5Layers(len(X_train[0])).model\n",
    "model.fit(X_train, y_train, epochs=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "pre = np.argmax(y_pred, axis=1)\n",
    "act = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f060f661",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a86272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out so I dont run by accident\n",
    "# Pretty bad performance\n",
    "\n",
    "# clf = svm.SVC(verbose=True)\n",
    "# clf.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# pre = clf.predict(X_test)\n",
    "# act = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a326212",
   "metadata": {},
   "source": [
    "## Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0731b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing emotions\n",
    "# X_abl = np.append(X, np.random.random(dual_emotion_features.shape), axis=1)\n",
    "\n",
    "# Replacing sBert\n",
    "X_abl = np.append(np.random.random(X.shape), dual_emotion_features, axis=1)\n",
    "\n",
    "y = pd.get_dummies(df['fn']).to_numpy(dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9247f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_abl, y, test_size=0.2)\n",
    "\n",
    "states, counts = np.unique(y_train, axis=0, return_counts=True)\n",
    "\n",
    "print('Train size:', len(y_train),'dist:', counts, '\\nTest size:', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9bee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train balanced:\n",
    "states, counts = np.unique(y_train, axis=0, return_counts=True)\n",
    "\n",
    "smallest_state = np.argmin(states)\n",
    "\n",
    "for i in range(len(states)):\n",
    "    if i == smallest_state:\n",
    "        continue\n",
    "    \n",
    "    new_y_train = y_train.copy()\n",
    "    new_X_train = X_train.copy()\n",
    "    \n",
    "    mask = np.all(y_train != states[i], axis=1)\n",
    "    new_y_train = new_y_train[mask]\n",
    "    new_X_train = new_X_train[mask]\n",
    "    \n",
    "    mask = np.all(y_train == states[i], axis=1)\n",
    "    y_examples = y_train[mask]\n",
    "    X_examples = X_train[mask]\n",
    "    \n",
    "    permutation = np.random.permutation(len(y_examples))\n",
    "    \n",
    "    # readd the trimmed down examples to y_train and X_train\n",
    "    new_y_train = np.concatenate((new_y_train, (y_examples[permutation])[:counts[smallest_state]]), axis=0)\n",
    "    y_train = new_y_train\n",
    "    \n",
    "    new_X_train = np.concatenate((new_X_train, (X_examples[permutation])[:counts[smallest_state]]), axis=0)\n",
    "    X_train = new_X_train\n",
    "    \n",
    "    # add the remaining examples to the testing arrays\n",
    "#     y_test = np.concatenate((y_test, (y_examples[permutation])[counts[smallest_state]:]), axis=0)\n",
    "#     X_test = np.concatenate((X_test, (X_examples[permutation])[counts[smallest_state]:]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f3590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "states, tr_counts = np.unique(y_train, axis=0, return_counts=True)\n",
    "states, te_counts = np.unique(y_test, axis=0, return_counts=True)\n",
    "\n",
    "print('Train size:', len(y_train),'dist:', tr_counts,\n",
    "      '\\nTest size:', len(y_test), 'dist:', te_counts)\n",
    "\n",
    "# model = MLP5Layers(len(X_train[0])).model\n",
    "model.fit(X_train, y_train, epochs=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfb3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "pre = np.argmax(y_pred, axis=1)\n",
    "act = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f8ed31",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2663ba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('F1:',  (f1_score(y_true=act, y_pred=pre)*100).round(2))\n",
    "print('Acc:', (accuracy_score(y_true=act, y_pred=pre)*100).round(2))\n",
    "print('Recall:', (recall_score(y_true=act, y_pred=pre)*100).round(2))\n",
    "print('Precision:', (precision_score(y_true=act, y_pred=pre)*100).round(2))\n",
    "\n",
    "confusion_matrix(y_true=act, y_pred=pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d81073",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d63766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 1.03, 0.025)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "for ind, val in enumerate(x[1:]):\n",
    "    m = (y_pred[:, 0] >= x[ind-1]) & (y_pred[:, 0] < val)\n",
    "    t_rn = y_pred[:, 0][m & (act == 0)]\n",
    "    t_fn = y_pred[:, 0][m & (act == 1)]\n",
    "    plt.bar(x=(x[ind-1] + val)/2 -0.005, height=len(t_fn), width=0.01, align='center', color='darkred', alpha=0.6)\n",
    "    plt.bar(x=(x[ind-1] + val)/2 +0.005, height=len(t_rn), width=0.01, align='center', color='darkblue', alpha=0.6)\n",
    "\n",
    "plt.legend(['Fake-news', 'Real-news'])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Real news probability in percent')\n",
    "plt.ylabel('#Posts')\n",
    "plt.title('Model predictions for real and fake news posts\\nusing mini sBert and dual-emotion features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87916e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 1.03, 0.025)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "rn_norm = sum(act == 0)\n",
    "fn_norm = sum(act == 1)\n",
    "\n",
    "for ind, val in enumerate(x[1:]):\n",
    "    m = (y_pred[:, 0] >= x[ind-1]) & (y_pred[:, 0] < val)\n",
    "    t_rn = y_pred[:, 0][m & (act == 0)]\n",
    "    t_fn = y_pred[:, 0][m & (act == 1)]\n",
    "    plt.bar(x=(x[ind-1] + val)/2 - 0.005, height=len(t_fn)/fn_norm, width=0.01, align='center', color='darkred', alpha=0.6)\n",
    "    plt.bar(x=(x[ind-1] + val)/2 + 0.005, height=len(t_rn)/rn_norm, width=0.01, align='center', color='darkblue', alpha=0.6)\n",
    "\n",
    "plt.legend(['Fake-news', 'Real-news'])\n",
    "plt.xlabel('Real news probability in percent')\n",
    "plt.ylabel('Post density in percent')\n",
    "plt.title('Model predictions for real and fake news posts\\nusing mini sBert and dual-emotion features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1a4d3",
   "metadata": {},
   "source": [
    "F1: 67.12\n",
    "Acc: 65.76\n",
    "Recall: 76.7\n",
    "Precision: 59.66\n",
    "array([[1184,  908],\n",
    "       [ 408, 1343]], dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b5bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']\n",
    "\n",
    "# data -> entry <-> emotion.\n",
    "# entry -> 0: rn cc, 1: rn ncc, 2: fn cc, 3: fn ncc\n",
    "\n",
    "# real data:\n",
    "# cc -> >0.8, ncc -> <0.2\n",
    "# grouping by real and fake first.\n",
    "# extract indices of X vectors\n",
    "# -> find correnct emotion positions\n",
    "\n",
    "pred = y_pred[:, 0]\n",
    "# pred is the real_news confidence as in the plots above\n",
    "# act is still the known factroid notation\n",
    "# kind of unintuitive, i know\n",
    "\n",
    "## variable naming explaiation\n",
    "## cc: correctly_classified, ncc: not_correctly_classified\n",
    "## rn: real_news, fn: fake-news\n",
    "\n",
    "# get all cc rn post with score < 0.1\n",
    "# -> model is really confident it's real-news and is correct\n",
    "rn_cc = X_test[(pred > 0.9) & (act == 0)]\n",
    "\n",
    "# get all ncc rn post with score < 0.1\n",
    "# -> model is really confident it's real-news and is not correct\n",
    "rn_ncc = X_test[(pred < 0.1) & (act == 0)]\n",
    "\n",
    "# get all cc fn post with score < 0.1\n",
    "# -> model is really confident it's fake-news and is correct\n",
    "fn_cc = X_test[(pred < 0.1) & (act == 1)]\n",
    "\n",
    "# get all cc rn post with score < 0.1\n",
    "# -> model is really confident it's fake-news and is not corrct\n",
    "fn_ncc = X_test[(pred > 0.9) & (act == 1)]\n",
    "\n",
    "print('Sizes:', '\\nrn_cc:', len(rn_cc), '\\nrn_ncc:',\n",
    "      len(rn_ncc), '\\nfn_cc', len(fn_cc), '\\nfn_ncc:', len(fn_ncc))\n",
    "\n",
    "# it makes sense that there are more rn_ncc than fn_cc. See first log-scale plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221dfe7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all entries in rn_cc, ... have 654-dim\n",
    "# 0-383 -> miniBert\n",
    "# 384-654 -> emotion vectors\n",
    "\n",
    "# mean of everything\n",
    "rn_cc = np.mean(rn_cc, axis=0)\n",
    "rn_ncc = np.mean(rn_ncc, axis=0)\n",
    "fn_cc = np.mean(fn_cc, axis=0)\n",
    "fn_ncc = np.mean(fn_ncc, axis=0)\n",
    "\n",
    "# 392:400 -> publisher emotion probs\n",
    "# 446:454 -> mean com\n",
    "# 500:508 -> max com\n",
    "# 554:562 -> mean gap\n",
    "# 608:616 -> max gap\n",
    "data = []\n",
    "groups = ['publisher', 'mean_com', 'max_com', 'mean_gap', 'max_gap']\n",
    "emotions = []\n",
    "for i, emotion in enumerate(base_emotions):\n",
    "    for j, name in enumerate(groups):\n",
    "        emotions.append(name + '_' + emotion)\n",
    "        data.append([rn_cc[392 + i + j*54],\n",
    "                rn_ncc[392 + i + j*54],\n",
    "                fn_cc[392 + i + j*54],\n",
    "                fn_ncc[392 + i + j*54]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9327e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(16,9))\n",
    "\n",
    "x_ticks = [-0.7, -0.5, -0.3, -0.1, 0.1, 0.3, 0.5, 0.7]\n",
    "\n",
    "for ind, emotion in enumerate(base_emotions):\n",
    "    \n",
    "    ax[ind//4, ind%4].set_title(emotion)\n",
    "    ax[ind//4, ind%4].set_xticks(x_ticks)\n",
    "    ax[ind//4, ind%4].set_yticks([])\n",
    "    ax[ind//4, ind%4].set_xticklabels(np.abs(x_ticks))\n",
    "    \n",
    "    ax[ind//4, ind%4].xaxis.set_minor_locator(MultipleLocator(0.05))\n",
    "    \n",
    "    ax[ind//4, ind%4].set_xlim([-0.701, 0.701])\n",
    "    ax[ind//4, ind%4].set_ylim([-4.3, 0.7])\n",
    "    \n",
    "    ax[ind//4, ind%4].grid(visible=True, axis='x')\n",
    "    \n",
    "    ax[ind//4, ind%4].spines['top'].set_visible(False)\n",
    "    ax[ind//4, ind%4].spines['right'].set_visible(False)\n",
    "    ax[ind//4, ind%4].spines['left'].set_visible(False)\n",
    "    \n",
    "    \n",
    "    for jnd, cat in enumerate(groups):\n",
    "        #print(ind, ind//4, ind%4)\n",
    "        # rn cc bar\n",
    "        rnc, = ax[ind//4, ind%4].barh(y=-jnd+0.1, width=abs(data[ind*5 + jnd][0]), height=0.15, color='royalblue')\n",
    "\n",
    "        # rn ncc bar\n",
    "        rnnc, = ax[ind//4, ind%4].barh(y=-jnd-0.1, width=abs(data[ind*5 + jnd][1]), height=0.15, color='navy')\n",
    "        \n",
    "        \n",
    "        # fn cc bar\n",
    "        fnc, = ax[ind//4, ind%4].barh(y=-jnd+0.1, width=-abs(data[ind*5 + jnd][2]), height=0.15, color='orangered')\n",
    "\n",
    "        # fn ncc bar\n",
    "        fnnc, = ax[ind//4, ind%4].barh(y=-jnd-0.1, width=-abs(data[ind*5 + jnd][3]), height=0.15, color='darkred')\n",
    "\n",
    "        # seperator line\n",
    "        ax[ind//4, ind%4].plot([0, 0], [-jnd-0.22, -jnd+0.22], color='black')\n",
    "\n",
    "        # text\n",
    "        ax[ind//4, ind%4].text(0, -jnd+0.35, cat, ha='center', fontsize=9)\n",
    "    \n",
    "fig.legend([fnc, fnnc, rnc, rnnc], ['Fake News - Correct', 'Fake News - Not Correct', 'Real News - Correct', 'Real News - Not Correct'])\n",
    "plt.suptitle('Feature weights in Correctly and not Correctly Classified Posts\\ngrouped by emotions', fontsize=20)\n",
    "\n",
    "plt.savefig('Emotion_dist.pdf', bbox_inches='tight')\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
