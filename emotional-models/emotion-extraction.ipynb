{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbaa108c",
   "metadata": {},
   "source": [
    "This notebook is based on the ['Mining Dual Emotion for Fake News Detection'](https://dl.acm.org/doi/pdf/10.1145/3442381.3450004) paper and their [emotion extraction code](https://github.com/RMSnow/WWW2021/blob/master/code/emotion/extract_emotion_en.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2ecee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84393f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== Category ==============================\n",
    "\n",
    "# Need to find a way to extract the nvidia emotions\n",
    "# TODO: understand how to use https://github.com/NVIDIA/Megatron-LM\n",
    "\n",
    "nvidia_emotions = ['anger', 'anticipation', 'disgust',\n",
    "                   'fear', 'joy', 'sadness', 'surprise', 'trust']\n",
    "nvidia_emotions.sort()\n",
    "\n",
    "def nvidia_arr(emotions_labels_dict=None, emotions_probs_dict=None):\n",
    "    arr = np.zeros(len(nvidia_emotions)*2)\n",
    "\n",
    "    if emotions_labels_dict is None or emotions_probs_dict is None:\n",
    "        return arr\n",
    "\n",
    "    for i, e in enumerate(nvidia_emotions):\n",
    "        arr[i] = emotions_labels_dict[e]\n",
    "        arr[i+len(nvidia_emotions)] = emotions_probs_dict[e]\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb93ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== Lexicon and Intensity ==============================\n",
    "\n",
    "# load negation words\n",
    "negation_words = []\n",
    "with open('./resources/English/others/negative/negationWords.txt', 'r') as src:\n",
    "    lines = src.readlines()\n",
    "    for line in lines:\n",
    "        negation_words.append(line.strip())\n",
    "\n",
    "print('\\nThe num of negation words: ', len(negation_words))\n",
    "\n",
    "# load degree words\n",
    "how_words_dict = dict()\n",
    "with open('./resources/English/HowNet/intensifierWords.txt', 'r') as src:\n",
    "    lines = src.readlines()\n",
    "    for line in lines:\n",
    "        how_word = line.strip().split()\n",
    "        how_words_dict[' '.join(how_word[:-1])] = float(how_word[-1])\n",
    "\n",
    "print('The num of degree words: ', len(how_words_dict),\n",
    "      '. eg: ', list(how_words_dict.items())[0])\n",
    "\n",
    "\n",
    "# negation value and degree value\n",
    "def get_not_and_how_value(cut_words, i, windows):\n",
    "    not_cnt = 0\n",
    "    how_v = 1\n",
    "\n",
    "    left = 0 if (i - windows) < 0 else (i - windows)\n",
    "    window_text = ' '.join(cut_words[left:i])\n",
    "\n",
    "    for w in negation_words:\n",
    "        if w in window_text:\n",
    "            not_cnt += 1\n",
    "    for w in how_words_dict.keys():\n",
    "        if w in window_text:\n",
    "            how_v *= how_words_dict[w]\n",
    "\n",
    "    return (-1) ** not_cnt, how_v\n",
    "\n",
    "\n",
    "lexicon_categories, lexicon_terms2arr = joblib.load(\n",
    "    './resources/English/NRC/preprocess/preprocess-lexicon.pkl')\n",
    "print('[NRC Lexicon]\\tThere are {} words, including {} categories, every term\\'s dimension is {}'.format(\n",
    "    len(lexicon_terms2arr), len(lexicon_categories), lexicon_terms2arr['happy'].shape))\n",
    "\n",
    "intensity_categories, intensity_terms2arr = joblib.load(\n",
    "    './resources/English/NRC/preprocess/preprocess-intensity.pkl')\n",
    "print('[NRC Intensity]\\tThere are {} words, including {} categories, every term\\'s dimension is {}'.format(\n",
    "    len(intensity_terms2arr), len(intensity_categories), intensity_terms2arr['happy'].shape))\n",
    "\n",
    "nrc_emotion_words = set(lexicon_terms2arr.keys()).union(\n",
    "    set(intensity_terms2arr.keys()))\n",
    "\n",
    "\n",
    "def nrc_arr(cut_words, windows=4):\n",
    "    arr = np.zeros(len(lexicon_categories) + len(intensity_categories))\n",
    "\n",
    "    for i, word in enumerate(cut_words):\n",
    "        if word in nrc_emotion_words:\n",
    "            not_v, how_v = get_not_and_how_value(cut_words, i, windows)\n",
    "\n",
    "            if word in lexicon_terms2arr:\n",
    "                arr[:len(lexicon_categories)] += not_v * \\\n",
    "                    how_v * lexicon_terms2arr[word]\n",
    "            if word in intensity_terms2arr:\n",
    "                arr[len(lexicon_categories):] += not_v * \\\n",
    "                    how_v * intensity_terms2arr[word]\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aea3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== Sentiment Scores ==============================\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_score(text):\n",
    "    scores = sentiment_analyzer.polarity_scores(text)\n",
    "    return scores['pos'], scores['neg'], scores['neu'], scores['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6882fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== Auxilary Features ==============================\n",
    "\n",
    "# Emoticon\n",
    "def isEmoji(content):\n",
    "    if not content:\n",
    "        return False\n",
    "    if u\"\\U0001F600\" <= content and content <= u\"\\U0001F64F\":\n",
    "        return True\n",
    "    elif u\"\\U0001F300\" <= content and content <= u\"\\U0001F5FF\":\n",
    "        return True\n",
    "    elif u\"\\U0001F680\" <= content and content <= u\"\\U0001F6FF\":\n",
    "        return True\n",
    "    elif u\"\\U0001F1E0\" <= content and content <= u\"\\U0001F1FF\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def emoji_count(text):\n",
    "    emoji = 0\n",
    "    for c in text:\n",
    "        if isEmoji(c):\n",
    "            emoji += 1\n",
    "    return emoji / len(text)\n",
    "\n",
    "smiling_emoticons = [':-)', ':)', ':o)', ':],' ':3',\n",
    "                     ':c)', ':>' '=]', '8)', '=)', ':}', ':^)', ':っ)']\n",
    "frowning_emoticons = [\n",
    "    '>:[', ':-(', ':(', ':-c', ':c', ':-<', ':っC', ':<', ':-[', ':[', ':{']\n",
    "\n",
    "\n",
    "def emoticon_arr(text):\n",
    "    smiling = 0\n",
    "    frowning = 0\n",
    "    for s in smiling_emoticons:\n",
    "        smiling += text.count(s)\n",
    "    for f in frowning_emoticons:\n",
    "        frowning += text.count(f)\n",
    "    return smiling / len(text), frowning / len(text), emoji_count(text)\n",
    "\n",
    "\n",
    "# Punctuation\n",
    "def symbols_count(text):\n",
    "    excl = (text.count('!') + text.count('！')) / len(text)\n",
    "    ques = (text.count('?') + text.count('？')) / len(text)\n",
    "    comma = (text.count(',') + text.count('，')) / len(text)\n",
    "    dot = (text.count('.') + text.count('。')) / len(text)\n",
    "    ellip = (text.count('..') + text.count('。。')) / len(text)\n",
    "\n",
    "    return excl, ques, comma, dot, ellip\n",
    "\n",
    "\n",
    "# Sentimental Words\n",
    "def init_words(file):\n",
    "    with open(file, 'r', encoding='utf-8') as src:\n",
    "        words = src.readlines()\n",
    "        words = [l.strip() for l in words]\n",
    "    print('File: {}, Words_sz = {}'.format(file.split('/')[-1], len(words)))\n",
    "    return list(set(words))\n",
    "\n",
    "pos_words = init_words('./resources/English/HowNet/正面情感词语（英文）.txt')\n",
    "pos_words += init_words('./resources/English/HowNet/正面评价词语（英文）.txt')\n",
    "neg_words = init_words('./resources/English/HowNet/负面情感词语（英文）.txt')\n",
    "neg_words += init_words('./resources/English/HowNet/负面评价词语（英文）.txt')\n",
    "\n",
    "pos_words = set(pos_words)\n",
    "neg_words = set(neg_words)\n",
    "print('[HowNet]\\tThere are {} positive words and {} negative words'.format(\n",
    "    len(pos_words), len(neg_words)))\n",
    "\n",
    "\n",
    "def sentiment_words_count(cut_words):\n",
    "    if len(cut_words) == 0:\n",
    "        return [0, 0, 0, 0]\n",
    "\n",
    "    # positive and negative words\n",
    "    sentiment = []\n",
    "    for words in [pos_words, neg_words]:\n",
    "        c = 0\n",
    "        for word in words:\n",
    "            if word in cut_words:\n",
    "                # print(word)\n",
    "                c += 1\n",
    "        sentiment.append(c)\n",
    "    sentiment = [c / len(cut_words) for c in sentiment]\n",
    "\n",
    "    # degree words\n",
    "    degree = 0\n",
    "    for word in how_words_dict:\n",
    "        if word in cut_words:\n",
    "            # print(word)\n",
    "            degree += how_words_dict[word]\n",
    "\n",
    "    # negation words\n",
    "    negation = 0\n",
    "    for word in negation_words:\n",
    "        negation += cut_words.count(word)\n",
    "    negation /= len(cut_words)\n",
    "\n",
    "    sentiment += [degree, negation]\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "# Personal Pronoun\n",
    "first_pronoun = init_words(\n",
    "    './resources/English/others/pronoun/1-personal-pronoun.txt')\n",
    "second_pronoun = init_words(\n",
    "    './resources/English/others/pronoun/2-personal-pronoun.txt')\n",
    "third_pronoun = init_words(\n",
    "    './resources/English/others/pronoun/3-personal-pronoun.txt')\n",
    "pronoun_words = [first_pronoun, second_pronoun, third_pronoun]\n",
    "\n",
    "\n",
    "def pronoun_count(cut_words):\n",
    "    if len(cut_words) == 0:\n",
    "        return [0, 0, 0]\n",
    "\n",
    "    pronoun = []\n",
    "    for words in pronoun_words:\n",
    "        c = 0\n",
    "        for word in words:\n",
    "            c += cut_words.count(word)\n",
    "        pronoun.append(c)\n",
    "\n",
    "    return [c / len(cut_words) for c in pronoun]\n",
    "\n",
    "# others\n",
    "\n",
    "\n",
    "def upper_letter_count(text):\n",
    "    upper = 0\n",
    "    for c in text:\n",
    "        if c.isupper():\n",
    "            upper += 1\n",
    "    return upper / len(text)\n",
    "\n",
    "\n",
    "# Auxilary Features\n",
    "def auxilary_features(text, cut_words):\n",
    "    arr = np.zeros(16)\n",
    "\n",
    "    arr[:3] = emoticon_arr(text)\n",
    "    arr[3:8] = symbols_count(text)\n",
    "    arr[8:12] = sentiment_words_count(cut_words)\n",
    "    arr[12:15] = pronoun_count(cut_words)\n",
    "    arr[15:16] = upper_letter_count(text)\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faaf179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== Main ==============================\n",
    "\n",
    "def del_url_at(text):\n",
    "    pattern = re.compile(\n",
    "        'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    urls = re.findall(pattern, text)\n",
    "    for url in urls:\n",
    "        text = text.replace(url, '')\n",
    "\n",
    "    pattern = re.compile(\n",
    "        '@(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    ats = re.findall(pattern, text)\n",
    "    for at in ats:\n",
    "        text = text.replace(at, '')\n",
    "\n",
    "    text = text.replace('', '').replace('\\r', '').replace('\\t', '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def cut_words_from_text(text):\n",
    "    pattern = r\"\"\"(?x)                   # set flag to allow verbose regexps \n",
    "                  (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "                  |\\d+(?:\\.\\d+)?%?       # numbers, incl. currency and percentages \n",
    "                  |\\w+(?:[-']\\w+)*       # words w/ optional internal hyphens/apostrophe \n",
    "                  |\\.\\.\\.                # ellipsis \n",
    "                  |(?:[.,;\"'?():-_`!])    # special characters with meanings \n",
    "                \"\"\"\n",
    "\n",
    "    return nltk.regexp_tokenize(del_url_at(text), pattern) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ef60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emotion(text):\n",
    "    cut_words = cut_words_from_text(text)\n",
    "\n",
    "    arr = np.zeros(54)\n",
    "\n",
    "    # the nvidia features may need more space due to newer version\n",
    "    arr[:16] = nvidia_arr(emotions_labels_dict, emotions_probs_dict)\n",
    "    \n",
    "    arr[16:34] = nrc_arr(cut_words)\n",
    "    arr[34:38] = sentiment_score(text)\n",
    "    arr[38:54] = auxilary_features(text, cut_words)\n",
    "\n",
    "    return arr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
