{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f08207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import texthero\n",
    "from scipy.stats import poisson\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/reddit_corpus_balanced_filtered.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b71ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "rn_post_length_words = {} # Stores {<length of post>: #occourence, ...}\n",
    "fn_post_length_words = {} # Stores {<length of post>: #occourence, ...}\n",
    "all_post_length_words = {} # Stores {<length of post>: #occourence, ...}\n",
    "\n",
    "for row in tqdm(df.iterrows(), total = len(df)):\n",
    "    data = row[1]\n",
    "    documents = data['documents']\n",
    "    \n",
    "    for doc_id, text, date, sub_reddit, labels in documents:\n",
    "        if len(labels) == 1:\n",
    "            is_fake_news = (labels[0][1] == 1)\n",
    "            \n",
    "            # clean text\n",
    "            t = texthero.clean(pd.Series(text))[0]\n",
    "            \n",
    "            n_words = len(t)\n",
    "            \n",
    "            if is_fake_news:\n",
    "                if n_words not in fn_post_length_words:\n",
    "                    fn_post_length_words[n_words] = 0\n",
    "                fn_post_length_words[n_words] += 1\n",
    "            \n",
    "            else:\n",
    "                if n_words not in rn_post_length_words:\n",
    "                    rn_post_length_words[n_words] = 0\n",
    "                rn_post_length_words[n_words] += 1\n",
    "            \n",
    "            if n_words not in all_post_length_words:\n",
    "                all_post_length_words[n_words] = 0\n",
    "            all_post_length_words[n_words] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8f4392",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = rn_post_length_words # change dic here\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "n = sum(dic.values())\n",
    "x_axis = sorted(dic)\n",
    "y_axis = np.zeros(len(x_axis))\n",
    "s = 0\n",
    "for ind, val in enumerate(x_axis):\n",
    "    s += dic[val]\n",
    "    y_axis[ind] = dic[val]\n",
    "    \n",
    "    \n",
    "plt.scatter(x_axis, y_axis, marker='x')\n",
    "plt.xlim([10,1600])\n",
    "plt.title('Distribution in all labeled posts')\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Number of posts')\n",
    "plt.savefig('./overview/word_length_dist.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "for dic in [all_post_length_words, fn_post_length_words, rn_post_length_words]:\n",
    "    n = sum(dic.values())\n",
    "    x_axis = sorted(dic)\n",
    "    y_axis = np.zeros(len(x_axis))\n",
    "    s = 0\n",
    "    for ind, val in enumerate(x_axis):\n",
    "        s += dic[val]\n",
    "        y_axis[ind] = s/n\n",
    "\n",
    "\n",
    "    plt.plot(x_axis, y_axis)\n",
    "plt.xlim([30,1600])\n",
    "#plt.xscale('log')\n",
    "plt.title('Cumulative Distribution')\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Number of posts')\n",
    "plt.legend(['all', 'fake news', 'real news'])\n",
    "plt.savefig('./overview/word_length_cum.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73890d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posts with multiple lables\n",
    "posts = {'x': [], 'y': []} # x: rn, y:fn\n",
    "\n",
    "for row in tqdm(df.iterrows(), total = len(df)):\n",
    "    data = row[1]\n",
    "    documents = data['documents']\n",
    "    \n",
    "    for doc_id, text, date, sub_reddit, labels in documents:\n",
    "        if len(labels) > 1:\n",
    "            dist = {'fn': 0, 'rn': 0}\n",
    "            for page, fn_news, bias, factual in labels:\n",
    "                if fn_news == 1:\n",
    "                    dist['fn'] += 1\n",
    "                else:\n",
    "                    dist['rn'] += 1\n",
    "            posts['x'].append(dist['rn'])\n",
    "            posts['y'].append(dist['fn'])\n",
    "            \n",
    "# posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d65839",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.DataFrame(posts)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(p['x'], p['y'], marker='x')\n",
    "plt.xlabel('Real-News Links')\n",
    "plt.ylabel('Fake-News Links')\n",
    "plt.title('Distribution of fake and real News in posts with multiple Links')\n",
    "plt.savefig('./overview/multi_links.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
