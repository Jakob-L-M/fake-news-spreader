{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f08207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import texthero\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/reddit_corpus_balanced_filtered.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b71ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "rn_post_length_words = {} # Stores {<length of post>: #occourence, ...}\n",
    "fn_post_length_words = {} # Stores {<length of post>: #occourence, ...}\n",
    "all_post_length_words = {} # Stores {<length of post>: #occourence, ...}\n",
    "\n",
    "for row in tqdm(df.iterrows(), total = len(df)):\n",
    "    data = row[1]\n",
    "    documents = data['documents']\n",
    "    \n",
    "    for doc_id, text, date, sub_reddit, labels in documents:\n",
    "        \n",
    "        # only pick labeled posts\n",
    "        if len(labels) == 1:\n",
    "            is_fake_news = (labels[0][1] == 1)\n",
    "            \n",
    "            # clean text\n",
    "            t = texthero.clean(pd.Series(text))[0]\n",
    "            \n",
    "            n_words = len(t)\n",
    "            \n",
    "            if is_fake_news:\n",
    "                if n_words not in fn_post_length_words:\n",
    "                    fn_post_length_words[n_words] = 0\n",
    "                fn_post_length_words[n_words] += 1\n",
    "            \n",
    "            else:\n",
    "                if n_words not in rn_post_length_words:\n",
    "                    rn_post_length_words[n_words] = 0\n",
    "                rn_post_length_words[n_words] += 1\n",
    "            \n",
    "            if n_words not in all_post_length_words:\n",
    "                all_post_length_words[n_words] = 0\n",
    "            all_post_length_words[n_words] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store id sets\n",
    "rn_ids = set()\n",
    "fn_ids = set()\n",
    "\n",
    "for row in tqdm(df.iterrows(), total = len(df)):\n",
    "    data = row[1]\n",
    "    documents = data['documents']\n",
    "    \n",
    "    for doc_id, text, date, sub_reddit, labels in documents:\n",
    "        \n",
    "        # only pick labeled posts\n",
    "        if len(labels) == 1:\n",
    "            is_fake_news = (labels[0][1] == 1)\n",
    "            \n",
    "            if is_fake_news:\n",
    "                fn_ids.add(doc_id)\n",
    "            else:\n",
    "                rn_ids.add(doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b638ab0",
   "metadata": {},
   "source": [
    "### Bar chart polts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8f4392",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = rn_post_length_words # change dic here\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "n = sum(dic.values())\n",
    "x_axis = sorted(dic)\n",
    "y_axis = np.zeros(len(x_axis))\n",
    "s = 0\n",
    "for ind, val in enumerate(x_axis):\n",
    "    s += dic[val]\n",
    "    y_axis[ind] = dic[val]\n",
    "    \n",
    "    \n",
    "plt.scatter(x_axis, y_axis, marker='x')\n",
    "plt.xlim([10,1600])\n",
    "plt.title('Distribution in all labeled posts')\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Number of posts')\n",
    "plt.savefig('./overview/word_length_dist.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "for dic in [all_post_length_words, fn_post_length_words, rn_post_length_words]:\n",
    "    n = sum(dic.values())\n",
    "    x_axis = sorted(dic)\n",
    "    y_axis = np.zeros(len(x_axis))\n",
    "    s = 0\n",
    "    for ind, val in enumerate(x_axis):\n",
    "        s += dic[val]\n",
    "        y_axis[ind] = s/n\n",
    "\n",
    "\n",
    "    plt.plot(x_axis, y_axis)\n",
    "plt.xlim([30,1600])\n",
    "#plt.xscale('log')\n",
    "plt.title('Cumulative Distribution')\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Number of posts')\n",
    "plt.legend(['all', 'fake news', 'real news'])\n",
    "plt.savefig('./overview/word_length_cum.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73890d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posts with multiple lables\n",
    "posts = {'x': [], 'y': []} # x: rn, y:fn\n",
    "\n",
    "for row in tqdm(df.iterrows(), total = len(df)):\n",
    "    data = row[1]\n",
    "    documents = data['documents']\n",
    "    \n",
    "    for doc_id, text, date, sub_reddit, labels in documents:\n",
    "        if len(labels) > 1:\n",
    "            dist = {'fn': 0, 'rn': 0}\n",
    "            for page, fn_news, bias, factual in labels:\n",
    "                if fn_news == 1:\n",
    "                    dist['fn'] += 1\n",
    "                else:\n",
    "                    dist['rn'] += 1\n",
    "            posts['x'].append(dist['rn'])\n",
    "            posts['y'].append(dist['fn'])\n",
    "            \n",
    "# posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e61734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.DataFrame(posts)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(p['x'], p['y'], marker='x')\n",
    "plt.xlabel('Real-News Links')\n",
    "plt.ylabel('Fake-News Links')\n",
    "plt.title('Distribution of fake and real News in posts with multiple Links')\n",
    "plt.savefig('./overview/multi_links.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148ec24",
   "metadata": {},
   "source": [
    "### Fake and Real News comment to post densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39705e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "comments = pd.read_pickle('../emotional-models/factroid_with_comments.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91440dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012bbbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make arrays from the dicts\n",
    "posts = []\n",
    "        \n",
    "for row in tqdm(comments.iterrows(), total=len(comments)):\n",
    "    data = row[1]\n",
    "    \n",
    "    doc_id = data['doc_id']\n",
    "    \n",
    "    temp = {'id': doc_id, 'text_len': len(data['text']), 'fn': 'Misinformation' if data['fn'] == 1 else 'Real News'}\n",
    "    \n",
    "    if len(data['comments']) > 0:\n",
    "        \n",
    "        com_lens = []\n",
    "        for com in data['comments']:\n",
    "             com_lens.append(len(data['comments'][com]))\n",
    "                \n",
    "        temp['com_len'] = np.mean(com_lens)\n",
    "        temp['com_num'] = len(com_lens)\n",
    "        \n",
    "    else:\n",
    "        temp['com_len'] = 0\n",
    "        temp['com_num'] = 0\n",
    "    posts.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6749412",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.DataFrame(posts)\n",
    "posts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ba757",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.histplot(data=posts_df[posts_df['fn']=='Real News'], x=\"com_len\", fill=True, alpha=0.5, binwidth=100, stat='percent', label='Real News')\n",
    "sns.histplot(data=posts_df[posts_df['fn']=='Misinformation'], x=\"com_len\", fill=True, alpha=0.5, binwidth=100, stat='percent', color='red', label='Misinformation')\n",
    "plt.yscale('log')\n",
    "plt.xlim([0, 10000])\n",
    "plt.legend()\n",
    "plt.xlabel('Mean Length')\n",
    "plt.title('Mean length of comments in all labeled Posts')\n",
    "plt.ylabel('Percent (log scale)')\n",
    "plt.savefig('./overview/mean-comment-length.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
