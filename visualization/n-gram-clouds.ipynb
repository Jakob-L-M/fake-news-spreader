{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faade78",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "correct_words = set(words.words())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_pos_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    "    \n",
    "def lemmatize(sentence):\n",
    "    res = \"\"\n",
    "    t = pos_tag(word_tokenize(sentence.lower()))\n",
    "    for i in t:\n",
    "        if i[0] in stop_words:\n",
    "            continue\n",
    "        if get_pos_tag(i[1]) is None:\n",
    "            if i[0] in correct_words:\n",
    "                res += i[0] + \" \"\n",
    "        else:\n",
    "            lem = wnl.lemmatize(i[0], pos=get_pos_tag(i[1]))\n",
    "            if (lem in correct_words):\n",
    "                res += lem + \" \"\n",
    "    return res[:-1]\n",
    "\n",
    "lemmatize(\"university hes the greatest, artist i have seen today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/reddit_corpus_balanced_filtered.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5fba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_groups = {}\n",
    "theme_groups['SARS-CoV-2'] = ['r/CovidVaccinated', 'r/Masks4All', 'r/NoLockdownsNoMasks', 'r/EndTheLockdowns', 'r/COVID19', 'r/COVID19positive', 'r/CoronavirusCanada', 'r/CoronavirusRecession', 'r/CoronavirusUK', 'r/CoronavirusUS', 'r/Coronavirus', 'r/LockdownSkepticism', 'r/NoNewNormal']\n",
    "theme_groups['Vaccines'] = ['r/CovidVaccinated', 'r/VACCINES', 'r/vaxxhappened', 'r/AntiVaxxers', 'r/antivax', 'r/TrueAntiVaccination', 'r/DebateVaccine', 'r/DebateVaccines']\n",
    "theme_groups['Abortion'] = ['r/AskProchoice', 'r/prochoice', 'r/insaneprolife', 'r/prolife', 'r/ProLifeLibertarians', 'r/Abortiondebate', 'r/abortion']\n",
    "theme_groups['womens-and-mens-rights'] = ['r/Feminism', 'r/feminisms', 'r/RadicalFeminism', 'r/RadicalFeminismUSA', 'r/MRActivism', 'r/MensRights', 'r/antifeminists', 'r/feminismformen', 'r/masculism', 'r/GenderCritical', 'r/Egalitarianism']\n",
    "theme_groups['Gun-control'] = ['r/Firearms', 'r/GunsAreCool', 'r/liberalgunowners', 'r/progun', 'r/guncontrol', 'r/GunDebates', 'r/GunResearch', 'r/gunpolitics']\n",
    "theme_groups['Climate-change'] = ['r/climateskeptics', 'r/GlobalClimateChange', 'r/climate', 'r/climatechange']\n",
    "theme_groups['5G'] = ['r/5GDebate']\n",
    "theme_groups['general-political-debate'] = ['r/JoeBiden', 'r/LeftistsForMen', 'r/Liberal', 'r/LockdownCriticalLeft', 'r/democrats', 'r/Conservative', 'r/ConservativesOnly', 'r/conservatives', 'r/Republican', 'r/RepublicanValues', 'r/politics', 'r/uspolitics', 'r/Impeach_Trump']\n",
    "\n",
    "inverse_theme_groups = {}\n",
    "for theme in theme_groups:\n",
    "    for sub in theme_groups[theme]:\n",
    "        inverse_theme_groups[sub] = theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461aba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "\n",
    "for row in tqdm(df.iterrows(), total=len(df)):\n",
    "    data = row[1]\n",
    "    documents = data['documents']\n",
    "    \n",
    "    for doc_id, text, date, sub_reddit, labels in documents:\n",
    "        dates.append(date.timestamp())\n",
    "\n",
    "times = np.array(sorted(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "times[-1] - times[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ddd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('lemma_post_dic.pkl', 'wb') as f:\n",
    "#     pickle.dump(sub_reddit_post_dic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b0ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemma_post_dic.pkl', 'rb') as f:\n",
    "    sub_reddit_post_dic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d23ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = []\n",
    "for topic in theme_groups:\n",
    "    res = []\n",
    "    for sub in theme_groups[topic]:\n",
    "        res.append(\" \".join(sub_reddit_post_dic[sub]))\n",
    "    M.append(\" \".join(res))\n",
    "len(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a709df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), min_df=3, stop_words='english')\n",
    "X = vectorizer.fit_transform(M)\n",
    "X_words = np.array(vectorizer.get_feature_names_out())\n",
    "print(len(X_words))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1950fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "\n",
    "topic_words = {}\n",
    "\n",
    "for ind, topic in enumerate(theme_groups):\n",
    "    arr = X[ind].toarray()[0]\n",
    "    words = X_words[arr != 0]\n",
    "    arr = arr[arr != 0]\n",
    "    top_k_ind = heapq.nlargest(k, enumerate(arr), key=lambda x: x[1])\n",
    "\n",
    "    # separate the wrights and normalize them\n",
    "    top_k_w = [i[1] for i in top_k_ind]\n",
    "    top_k_w = list(np.array(top_k_w)/sum(top_k_w))\n",
    "\n",
    "    topic_words[topic] = {}\n",
    "    for i in range(0, len(top_k_ind)):\n",
    "        # transform to uppercase for uniform appearance\n",
    "        topic_words[topic][words[top_k_ind[i][0]].upper()] = top_k_ind[i][1]\n",
    "        \n",
    "        #topic_words[topic]['weights'].append()\n",
    "\n",
    "#topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the cloud\n",
    "\n",
    "wc = WordCloud(width=1900,height=1000,relative_scaling=0.9,background_color='white',max_font_size = 2000)\n",
    "for topic in theme_groups:\n",
    "    cloud = wc.generate_from_frequencies(topic_words[topic])\n",
    "    plt.figure(figsize=(16,9))\n",
    "   \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(topic, fontsize=70)\n",
    "    plt.box = False\n",
    "    plt.imshow(cloud)\n",
    "    plt.savefig('./categories/' + topic + '_cloud.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
