{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faade78",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def get_pos_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    "    \n",
    "def lemmatize(sentence):\n",
    "    res = \"\"\n",
    "    t = pos_tag(word_tokenize(sentence))\n",
    "    for i in t:\n",
    "        if (len(i[0]) < 2):\n",
    "            continue\n",
    "        if ('www.' in i[0]):\n",
    "            continue\n",
    "        if(get_pos_tag(i[1]) is None):\n",
    "            res += i[0] + \" \"\n",
    "        else:\n",
    "            res += wnl.lemmatize(i[0], pos=get_pos_tag(i[1])) + \" \"\n",
    "    return res.lower()[:-1]\n",
    "\n",
    "lemmatize(\"Hes the greatest artrist I have seen today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/reddit_corpus_balanced_filtered.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5fba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_groups = {}\n",
    "theme_groups['SARS-CoV-2'] = ['r/CovidVaccinated', 'r/Masks4All', 'r/NoLockdownsNoMasks', 'r/EndTheLockdowns', 'r/COVID19', 'r/COVID19positive', 'r/CoronavirusCanada', 'r/CoronavirusRecession', 'r/CoronavirusUK', 'r/CoronavirusUS', 'r/Coronavirus', 'r/LockdownSkepticism', 'r/NoNewNormal']\n",
    "theme_groups['Vaccines'] = ['r/CovidVaccinated', 'r/VACCINES', 'r/vaxxhappened', 'r/AntiVaxxers', 'r/antivax', 'r/TrueAntiVaccination', 'r/DebateVaccine', 'r/DebateVaccines']\n",
    "theme_groups['Abortion'] = ['r/AskProchoice', 'r/prochoice', 'r/insaneprolife', 'r/prolife', 'r/ProLifeLibertarians', 'r/Abortiondebate', 'r/abortion']\n",
    "theme_groups['womens-and-mens-rights'] = ['r/Feminism', 'r/feminisms', 'r/RadicalFeminism', 'r/RadicalFeminismUSA', 'r/MRActivism', 'r/MensRights', 'r/antifeminists', 'r/feminismformen', 'r/masculism', 'r/GenderCritical', 'r/Egalitarianism']\n",
    "theme_groups['Gun-control'] = ['r/Firearms', 'r/GunsAreCool', 'r/liberalgunowners', 'r/progun', 'r/guncontrol', 'r/GunDebates', 'r/GunResearch', 'r/gunpolitics']\n",
    "theme_groups['Climate-change'] = ['r/climateskeptics', 'r/GlobalClimateChange', 'r/climate', 'r/climatechange']\n",
    "theme_groups['5G'] = ['r/5GDebate']\n",
    "theme_groups['general-political-debate'] = ['r/JoeBiden', 'r/LeftistsForMen', 'r/Liberal', 'r/LockdownCriticalLeft', 'r/democrats', 'r/Conservative', 'r/ConservativesOnly', 'r/conservatives', 'r/Republican', 'r/RepublicanValues', 'r/politics', 'r/uspolitics', 'r/Impeach_Trump']\n",
    "\n",
    "inverse_theme_groups = {}\n",
    "for theme in theme_groups:\n",
    "    for sub in theme_groups[theme]:\n",
    "        inverse_theme_groups[sub] = theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461aba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_reddit_post_dic = {}\n",
    "\n",
    "for row in tqdm(df.iterrows(), total=len(df)):\n",
    "    data = row[1]\n",
    "    documents = data['documents']\n",
    "    \n",
    "    for doc_id, text, date, sub_reddit, labels in documents:\n",
    "        if sub_reddit not in sub_reddit_post_dic:\n",
    "            sub_reddit_post_dic[sub_reddit] = []\n",
    "        sub_reddit_post_dic[sub_reddit].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d23ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = []\n",
    "for topic in theme_groups:\n",
    "    res = []\n",
    "    for sub in theme_groups[topic]:\n",
    "        res.append(\" \".join(sub_reddit_post_dic[sub]))\n",
    "    M.append(\" \".join(res))\n",
    "len(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a709df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(M)\n",
    "X_words = np.array(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1950fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(X[0])\n",
    "words = X_words[arr != 0]\n",
    "arr = arr[arr != 0]\n",
    "top_k_ind = heapq.nlargest(k, enumerate(arr), key=lambda x: x[1])\n",
    "        \n",
    "# separate the wrights and normalize them\n",
    "top_k_w = [i[1] for i in top_k_ind]\n",
    "top_k_w = list(np.array(top_k_w)/sum(top_k_w))\n",
    "\n",
    "temp = {'words': [], 'weights': []}\n",
    "for i in range(0, len(top_k_ind)):\n",
    "    # transform to uppercase for uniform appearance\n",
    "    temp['words'].append(words[top_k_ind[i][0]].upper())\n",
    "\n",
    "    # round to 5 digits to save space in export\n",
    "    temp['weights'].append(float(f'{top_k_w[i]:.5f}'))\n",
    "\n",
    "temp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
